{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping for Space Debris Information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Engines \n",
    "\n",
    "We are going to scrape several search engines for the latest space debris information includeing \n",
    "\n",
    "- Google \n",
    "- Bing\n",
    "- Yahoo!\n",
    "\n",
    "For each search engine we must probe how to get the searches for desired keyword, time frame, sources, and etc.\n",
    " _____________________________________________________________________________________________________________________________________________________\n",
    "#### [Google](https://google.com/)\n",
    "The base of the url for a Google search is \"search?\". Following this we enter our necessary url parameters.\n",
    "\n",
    "url parameters \n",
    "\n",
    "1. q : parameter where you enter the keyword to search\n",
    "2. tbm : choose to search for videos, news, etc.\n",
    "    - nws : news \n",
    "    - vid : video \n",
    "    - isch : image \n",
    "3. as_qdr : select a specific time frame such as \n",
    "    - d : past 24 hours \n",
    "    - w : past week\n",
    "    - m : past month\n",
    "    - y : past year \n",
    "4. start : select the page number (should be multiples of 10: 0, 10, 20, 30, ...)\n",
    "5. sourceid : this will be set to \"chrome\"\n",
    "6. ie : this selects the encoding which will be set to \"UTF-8\"\n",
    "\n",
    "So for an example, the url we must search for becomes \n",
    "\n",
    "https://google.com/search?q=space+debris&tbm=nws&as_qdr=w&start=0&sourceid=chrome&ie=UTF-8\n",
    "\n",
    " _____________________________________________________________________________________________________________________________________________________\n",
    "#### [Bing](https://bing.com)\n",
    "\n",
    "url parameters\n",
    "1. q : parameter where you enter the keyword to search\n",
    "2. news: enter in the url address after \".com/\" to filter only news sources \n",
    "3. qft : the time frame \n",
    "    - interval%3d%224%22 : past hour \n",
    "    - interval%3d%227%22 : past 24 hours \n",
    "    - interval%3d%228%22 : past 7 days \n",
    "    - interval%3d%229%22 : past 30 days \n",
    "\n",
    "So an example would be \n",
    "\n",
    "https://bing.com/news/search?q=space+debris&qft=interval%3d%228%22 \n",
    "\n",
    " _____________________________________________________________________________________________________________________________________________________\n",
    "#### [Yahoo!](https://www.yahoo.com/)\n",
    "\n",
    "Does not provide much news sources but the following would be an example of an url\n",
    "\n",
    "All results \n",
    "\n",
    "https://search.yahoo.com/search?p=space+debris \n",
    "\n",
    "News sources \n",
    "\n",
    "https://news.search.yahoo.com/search?p=space+debris\n",
    "\n",
    " _____________________________________________________________________________________________________________________________________________________\n",
    "### space.com\n",
    "\n",
    "Additionally we are going to scrape one good space news source called [space.com](https://space.com)\n",
    "\n",
    "url parameters\n",
    "1. /news : to fetch news sources \n",
    "2. /# : where # indicates a number and this number is the page number we want for the articles \n",
    "\n",
    "An example url would be \n",
    "\n",
    "https://space.com/news/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrape Examples \n",
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Search engine \n",
    "\n",
    "# Import necessary modules\n",
    "import requests \n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Google for space debris information \n",
    "# defining the parameters for the web-scraping \n",
    "params = ['q', 'tbm', 'as_qdr', 'start', 'sourceid', 'ie']\n",
    "param_dict = {}\n",
    "for p in params:\n",
    "    dialog = \"Enter the searching parameter for \" + p + \" -> \"\n",
    "    param_dict[p] = input(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the dictionary storing the parameter information \n",
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace space with + sign \n",
    "param_dict['q'] = param_dict['q'].replace(' ', '+')\n",
    "\n",
    "# print out to check the results \n",
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create url link with the dictionary information \n",
    "\n",
    "# When the value for a key in the dictionary is empty do not increment \n",
    "# a counter indicating how many empty keys are inside the dictionary \n",
    "i = 1\n",
    "ct = 0\n",
    "for v in param_dict.values():\n",
    "    if not v:\n",
    "        ct += 1\n",
    "\n",
    "# Create a base url for to search \n",
    "url = 'https://google.com/search?'\n",
    "# Create the url from the parameter information in the dictionary \n",
    "for k, v in param_dict.items():\n",
    "    if v:\n",
    "        url += k + '=' + v     \n",
    "        if i != len(param_dict)-ct:\n",
    "            url += '&'\n",
    "    i += 1\n",
    "\n",
    "# Check the url by printing it out \n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD1\n",
    "# urllib\n",
    "# import urllib.request \n",
    "# HEADERS={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "#          \"X-Requested-With\": \"XMLHttpRequest\"}\n",
    "# req = urllib.request.Request(url, headers=HEADERS)\n",
    "# page = urllib.request.urlopen(req)\n",
    "#soup = bs(page,\"html.parser\")\n",
    "\n",
    "# METHOD 2\n",
    "# Requests\n",
    "r = requests.get(url)\n",
    "r.encoding = 'utf-8'\n",
    "soup1 = bs(r.text, 'html.parser')\n",
    "r.close()\n",
    "# soup2 = soup1.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the links from the HTML retrieved by \n",
    "# beautiful soup\n",
    "divs = soup1.find_all('div', {\"class\", \"kCrYT\"})\n",
    "links = []\n",
    "for div in divs:\n",
    "    atag = div.a\n",
    "    if atag:\n",
    "        links.append(atag['href'])\n",
    "\n",
    "# Rinse/filter the urls by striping the unnecessary parts \n",
    "for idx, link in enumerate(links):\n",
    "    link = link.lstrip(\"/url?q=\")\n",
    "    link = link.replace(\"&sa=\", \" \")\n",
    "    link1, link2 = link.split()\n",
    "    links[idx] = link1\n",
    "\n",
    "# Print out the links \n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use requests to open each link and scrape each of them \n",
    "link1 = links[0]\n",
    "r = requests.get(link1)\n",
    "r.encoding = 'utf-8'\n",
    "soup = bs(r.text, 'html.parser')\n",
    "r.close()\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What do you want to search in Bing?  space debris\n",
      "Would you like to search for news sources or normal sources? Y[yes]/N[no] Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What time frame would you like to search for? \n",
      "(1) past hour\n",
      "(2) past 24 hours\n",
      "(3) past 7 days\n",
      "(4) past 30 days\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chose a number ->  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'q': 'space debris', 'news': 'Y', 'qft': '4'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scraping Google for space debris information \n",
    "# defining the parameters for the web-scraping \n",
    "params = ['q', 'news', 'qft']\n",
    "param_dict = {}\n",
    "param_dict['q'] = input(\"What do you want to search in Bing? \")\n",
    "param_dict['news'] = input(\"Would you like to search for news sources or normal sources? Y[yes]/N[no]\")\n",
    "print(\"What time frame would you like to search for? \")\n",
    "print(\"(1) past hour\")\n",
    "print('(2) past 24 hours')\n",
    "print('(3) past 7 days')\n",
    "print('(4) past 30 days')\n",
    "param_dict['qft'] = input(\"Chose a number -> \")    \n",
    "\n",
    "# print out the dictionary storing the parameter information \n",
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter the base url depending on the parameters \n",
    "if param_dict['news'] == \"Y\":\n",
    "    url = 'https://bing.com/news/search?'\n",
    "else:\n",
    "    url = 'https://bing.com/search?'\n",
    "    \n",
    "# Remove the 'news' key \n",
    "del param_dict['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the interval for the search depending on the parameters\n",
    "intervals = {\n",
    "    '1': 'interval%3d%224%22',\n",
    "    '2': 'interval%3d%227%22',\n",
    "    '3': 'interval%3d%228%22',\n",
    "    '4': 'interval%3d%229%22'\n",
    "}\n",
    "param_dict['qft'] = intervals[param_dict['qft']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q': 'space+debris', 'qft': 'interval%3d%229%22'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace space with + sign \n",
    "param_dict['q'] = param_dict['q'].replace(' ', '+')\n",
    "# print out to check the results \n",
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://bing.com/news/search?q=space+debris&qft=interval%3d%229%22'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create url link with the dictionary information \n",
    "\n",
    "# When the value for a key in the dictionary is empty do not increment \n",
    "# a counter indicating how many empty keys are inside the dictionary \n",
    "i = 1\n",
    "ct = 0\n",
    "for v in param_dict.values():\n",
    "    if not v:\n",
    "        ct += 1\n",
    "\n",
    "# Create the url from the parameter information in the dictionary \n",
    "for k, v in param_dict.items():\n",
    "    if v:\n",
    "        url += k + '=' + v     \n",
    "        if i != len(param_dict)-ct:\n",
    "            url += '&'\n",
    "    i += 1\n",
    "\n",
    "# Check the url by printing it out \n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD1\n",
    "# urllib\n",
    "# import urllib.request \n",
    "# HEADERS={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "#          \"X-Requested-With\": \"XMLHttpRequest\"}\n",
    "# req = urllib.request.Request(url, headers=HEADERS)\n",
    "# page = urllib.request.urlopen(req)\n",
    "#soup = bs(page,\"html.parser\")\n",
    "\n",
    "# METHOD 2\n",
    "# Requests\n",
    "r = requests.get(url)\n",
    "r.encoding = 'utf-8'\n",
    "soup1 = bs(r.text, 'html.parser')\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the links from the HTML retrieved by \n",
    "# beautiful soup\n",
    "divs = soup1.find_all('div', {\"class\", \"t_t\"})\n",
    "links = []\n",
    "for div in divs:\n",
    "    atag = div.a\n",
    "    if atag:\n",
    "        links.append(atag['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://tabi-labo.com/298313/wt-europe-clear-space',\n",
       " 'https://prtimes.jp/main/html/rd/p/000000008.000067481.html',\n",
       " 'https://www.iza.ne.jp/kiji/pressrelease/news/201222/prl20122213410247-n1.html',\n",
       " 'https://news.yahoo.co.jp/articles/4afb53ecca93714023a811807ff0006ab41ac6e8',\n",
       " 'https://news.toremaga.com/release/others/1682045.html',\n",
       " 'http://fabcross.jp/interview/20201217_astroscale.html',\n",
       " 'https://www.alterna.co.jp/34219/4/',\n",
       " 'https://www.minato-yamaguchi.co.jp/yama/e-yama/articles/19464',\n",
       " 'https://realsound.jp/movie/2020/12/post-679625.html',\n",
       " 'https://jp.techcrunch.com/2020/12/19/2020-12-18-orbital-refueling-and-manufacturing-go-from-theory-to-reality-in-2021/',\n",
       " 'http://www.jwing.net/news/33641',\n",
       " 'https://article.auone.jp/detail/1/2/2/101_2_r_20201217_1608162601724821']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use requests to open each link and scrape each of them \n",
    "link1 = links[0]\n",
    "r = requests.get(link1)\n",
    "r.encoding = 'utf-8'\n",
    "soup = bs(r.text, 'html.parser')\n",
    "r.close()\n",
    "print(soup.prettify())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
