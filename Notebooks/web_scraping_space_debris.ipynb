{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping for Space Debris Information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Engines \n",
    "\n",
    "We are going to scrape several search engines for the latest space debris information includeing \n",
    "\n",
    "- Google \n",
    "- Bing\n",
    "- Yahoo!\n",
    "\n",
    "For each search engine we must probe how to get the searches for desired keyword, time frame, sources, and etc.\n",
    " _____________________________________________________________________________________________________________________________________________________\n",
    "#### [Google](https://google.com/)\n",
    "The base of the url for a Google search is \"search?\". Following this we enter our necessary url parameters.\n",
    "\n",
    "url parameters \n",
    "\n",
    "1. q : parameter where you enter the keyword to search\n",
    "2. tbm : choose to search for videos, news, etc.\n",
    "    - nws : news \n",
    "    - vid : video \n",
    "    - isch : image \n",
    "3. as_qdr : select a specific time frame such as \n",
    "    - d : past 24 hours \n",
    "    - w : past week\n",
    "    - m : past month\n",
    "    - y : past year \n",
    "4. start : select the page number (should be multiples of 10: 0, 10, 20, 30, ...)\n",
    "5. sourceid : this will be set to \"chrome\"\n",
    "6. ie : this selects the encoding which will be set to \"UTF-8\"\n",
    "\n",
    "So for an example, the url we must search for becomes \n",
    "\n",
    "https://google.com/search?q=space+debris&tbm=nws&as_qdr=w&start=0&sourceid=chrome&ie=UTF-8\n",
    "\n",
    " _____________________________________________________________________________________________________________________________________________________\n",
    "#### [Bing](https://bing.com)\n",
    "\n",
    "url parameters\n",
    "1. q : parameter where you enter the keyword to search\n",
    "2. news: enter in the url address after \".com/\" to filter only news sources \n",
    "3. qft : the time frame \n",
    "    - interval%3d%224%22 : past hour \n",
    "    - interval%3d%227%22 : past 24 hours \n",
    "    - interval%3d%228%22 : past 7 days \n",
    "    - interval%3d%229%22 : past 30 days \n",
    "\n",
    "So an example would be \n",
    "\n",
    "https://bing.com/news/search?q=space+debris&qft=interval%3d%228%22 \n",
    "\n",
    " _____________________________________________________________________________________________________________________________________________________\n",
    "#### [Yahoo!](https://www.yahoo.com/)\n",
    "\n",
    "Does not provide much news sources but the following would be an example of an url\n",
    "\n",
    "All results \n",
    "\n",
    "https://search.yahoo.com/search?p=space+debris \n",
    "\n",
    "News sources \n",
    "\n",
    "https://news.search.yahoo.com/search?p=space+debris\n",
    "\n",
    " _____________________________________________________________________________________________________________________________________________________\n",
    "### space.com\n",
    "\n",
    "Additionally we are going to scrape one good space news source called [space.com](https://space.com)\n",
    "\n",
    "url parameters\n",
    "1. /news : to fetch news sources \n",
    "2. /# : where # indicates a number and this number is the page number we want for the articles \n",
    "\n",
    "An example url would be \n",
    "\n",
    "https://space.com/news/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrape Examples \n",
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Search engine \n",
    "\n",
    "# Import necessary modules\n",
    "import requests \n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the searching parameter for q ->  space debris\n",
      "Enter the searching parameter for tbm ->  nws\n",
      "Enter the searching parameter for as_qdr ->  d\n",
      "Enter the searching parameter for start ->  0\n",
      "Enter the searching parameter for sourceid ->  chrome\n",
      "Enter the searching parameter for ie ->  utf-8\n"
     ]
    }
   ],
   "source": [
    "# Scraping Google for space debris information \n",
    "\n",
    "params = ['q', 'tbm', 'as_qdr', 'start', 'sourceid', 'ie']\n",
    "param_dict = {}\n",
    "for p in params:\n",
    "    dialog = \"Enter the searching parameter for \" + p + \" -> \"\n",
    "    param_dict[p] = input(dialog)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q': 'space debris',\n",
       " 'tbm': 'nws',\n",
       " 'as_qdr': 'd',\n",
       " 'start': '0',\n",
       " 'sourceid': 'chrome',\n",
       " 'ie': 'utf-8'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q': 'space+debris',\n",
       " 'tbm': 'nws',\n",
       " 'as_qdr': 'd',\n",
       " 'start': '0',\n",
       " 'sourceid': 'chrome',\n",
       " 'ie': 'utf-8'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace space with + sign \n",
    "param_dict['q'] = param_dict['q'].replace(' ', '+')\n",
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create url link with the dictionary information \n",
    "\n",
    "i = 1\n",
    "ct = 0\n",
    "for v in param_dict.values():\n",
    "    if not v:\n",
    "        ct += 1\n",
    "        \n",
    "url = 'https://google.com/search?'\n",
    "for k, v in param_dict.items():\n",
    "    if v:\n",
    "        url += k + '=' + v     \n",
    "        if i != len(param_dict)-ct:\n",
    "            url += '&'\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://google.com/search?q=space+debris&tbm=nws&as_qdr=d&start=0&sourceid=chrome&ie=utf-8'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib\n",
    "# import urllib.request \n",
    "# HEADERS={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "#          \"X-Requested-With\": \"XMLHttpRequest\"}\n",
    "# req = urllib.request.Request(url, headers=HEADERS)\n",
    "# page = urllib.request.urlopen(req)\n",
    "\n",
    "# Requests\n",
    "r = requests.get(url)\n",
    "r.encoding = 'utf-8'\n",
    "soup1 = bs(r.text, 'html.parser')\n",
    "r.close()\n",
    "# soup2 = soup1.prettify()\n",
    "\n",
    "soup = bs(page,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup1.find_all('div', {\"class\", \"kCrYT\"})\n",
    "links = []\n",
    "for div in divs:\n",
    "    atag = div.a\n",
    "    if atag:\n",
    "        links.append(atag['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, link in enumerate(links):\n",
    "    link = link.lstrip(\"/url?q=\")\n",
    "    link = link.replace(\"&sa=\", \" \")\n",
    "    link1, link2 = link.split()\n",
    "    links[idx] = link1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://spaceref.com/news/viewsr.html%3Fpid%3D54396',\n",
       " 'http://spaceref.com/news/viewsr.html%3Fpid%3D54397',\n",
       " 'https://www.ign.com/articles/giant-claw-heading-into-orbit-space-junk-clean-up',\n",
       " 'https://www.ign.com/articles/giant-claw-heading-into-orbit-space-junk-clean-up',\n",
       " 'https://eurasiantimes.com/japan-to-reduce-space-junk-with-the-launch-of-worlds-first-wooden-satellite-by-2023/',\n",
       " 'https://eurasiantimes.com/japan-to-reduce-space-junk-with-the-launch-of-worlds-first-wooden-satellite-by-2023/',\n",
       " 'https://techxplore.com/news/2020-12-fukushima-nuclear-debris-virus.html',\n",
       " 'https://techxplore.com/news/2020-12-fukushima-nuclear-debris-virus.html',\n",
       " 'https://www.ign.com/articles/spooky-circles-in-space-are-puzzling-astronomers',\n",
       " 'https://www.ign.com/articles/spooky-circles-in-space-are-puzzling-astronomers',\n",
       " 'https://www.theday.com/real-estate/20201225/keep-your-floors-clean-this-winter',\n",
       " 'https://www.spacedaily.com/reports/Space_Electric_Thruster_System_SETS_to_Demonstrate_In_Space_Performance_as_Part_of_Firefly_Aerospaces_Debut_Alpha_Launch_999.html',\n",
       " 'https://www.space.com/the-expanse-season-5-episode-1-recap',\n",
       " 'https://www.space.com/the-expanse-season-5-episode-1-recap',\n",
       " 'http://www.chronline.com/northwest_regional_news/six-months-after-the-chop-seattles-cal-anderson-park-has-officially-reopened/article_72d364c2-463c-11eb-b27a-eb0d4dada7e9.html',\n",
       " 'http://www.chronline.com/northwest_regional_news/six-months-after-the-chop-seattles-cal-anderson-park-has-officially-reopened/article_72d364c2-463c-11eb-b27a-eb0d4dada7e9.html']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
